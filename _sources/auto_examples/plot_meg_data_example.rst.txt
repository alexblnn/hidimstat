
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_meg_data_example.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_meg_data_example.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_meg_data_example.py:


Support recovery on MEG data
============================

This example compares several methods that recover the support in the MEG/EEG
source localization problem with statistical guarantees. Here we work
with two datasets that study three different tasks (visual, audio, somato).

We reproduce the real data experiment of Chevalier et al. (2020) [1]_,
which shows the benefit of (ensemble) clustered inference such as
(ensemble of) clustered desparsified Multi-Task Lasso ((e)cd-MTLasso)
over standard approach such as sLORETA. Specifically, it retrieves
the support using a natural threshold (not computed a posteriori)
of the estimated parameter. The estimated support enjoys statistical
guarantees.

References
----------
.. [1] Chevalier, J. A., Gramfort, A., Salmon, J., & Thirion, B. (2020).
       Statistical control for spatio-temporal MEG/EEG source imaging with
       desparsified multi-task Lasso. In NeurIPS 2020-34h Conference on
       Neural Information Processing Systems.

.. GENERATED FROM PYTHON SOURCE LINES 24-42

.. code-block:: default


    import os
    import numpy as np
    import matplotlib.image as mpimg
    import matplotlib.pyplot as plt
    import mne
    from scipy.sparse.csgraph import connected_components
    from mne.datasets import sample, somato
    from mne.inverse_sparse.mxne_inverse import _prepare_gain, _make_sparse_stc
    from mne.minimum_norm import make_inverse_operator, apply_inverse
    from sklearn.cluster import FeatureAgglomeration
    from sklearn.metrics.pairwise import pairwise_distances

    from hidimstat.clustered_inference import clustered_inference
    from hidimstat.ensemble_clustered_inference import \
        ensemble_clustered_inference
    from hidimstat.stat_tools import zscore_from_pval








.. GENERATED FROM PYTHON SOURCE LINES 43-50

Specific preprocessing functions
--------------------------------
The functions below are used to load or preprocess the data or to put
the solution in a convenient format. If you are reading this example
for the first time, you should skip this section.

The following function loads the data from the sample dataset.

.. GENERATED FROM PYTHON SOURCE LINES 50-93

.. code-block:: default



    def _load_sample(cond):
        '''Load data from the sample dataset'''

        # Get data paths
        subject = 'sample'
        data_path = sample.data_path()
        fwd_fname_suffix = 'MEG/sample/sample_audvis-meg-eeg-oct-6-fwd.fif'
        fwd_fname = os.path.join(data_path, fwd_fname_suffix)
        ave_fname = os.path.join(data_path, 'MEG/sample/sample_audvis-ave.fif')
        cov_fname_suffix = 'MEG/sample/sample_audvis-shrunk-cov.fif'
        cov_fname = os.path.join(data_path, cov_fname_suffix)
        cov_fname = data_path + '/' + cov_fname_suffix
        subjects_dir = os.path.join(data_path, 'subjects')

        if cond == 'audio':
            condition = 'Left Auditory'
        elif cond == 'visual':
            condition = 'Left visual'

        # Read noise covariance matrix
        noise_cov = mne.read_cov(cov_fname)

        # Read forward matrix
        forward = mne.read_forward_solution(fwd_fname)

        # Handling average file
        evoked = mne.read_evokeds(ave_fname, condition=condition,
                                  baseline=(None, 0))
        evoked = evoked.pick_types('grad')

        # Selecting relevant time window
        evoked.plot()
        t_min, t_max = 0.05, 0.1
        t_step = 0.01

        pca = False

        return (subject, subjects_dir, noise_cov, forward, evoked,
                t_min, t_max, t_step, pca)









.. GENERATED FROM PYTHON SOURCE LINES 94-95

The next function loads the data from the somato dataset.

.. GENERATED FROM PYTHON SOURCE LINES 95-141

.. code-block:: default



    def _load_somato(cond):
        '''Load data from the somato dataset'''

        # Get data paths
        data_path = somato.data_path()
        subject = '01'
        subjects_dir = data_path + '/derivatives/freesurfer/subjects'
        raw_fname = os.path.join(data_path, f'sub-{subject}', 'meg',
                                 f'sub-{subject}_task-{cond}_meg.fif')
        fwd_fname = os.path.join(data_path, 'derivatives', f'sub-{subject}',
                                 f'sub-{subject}_task-{cond}-fwd.fif')

        # Read evoked
        raw = mne.io.read_raw_fif(raw_fname)
        events = mne.find_events(raw, stim_channel='STI 014')
        reject = dict(grad=4000e-13, eog=350e-6)
        picks = mne.pick_types(raw.info, meg=True, eeg=True, eog=True)

        event_id, tmin, tmax = 1, -.2, .25
        epochs = mne.Epochs(raw, events, event_id, tmin, tmax, picks=picks,
                            reject=reject, preload=True)
        evoked = epochs.average()
        evoked = evoked.pick_types('grad')
        # evoked.plot()

        # Compute noise covariance matrix
        noise_cov = mne.compute_covariance(epochs, rank='info', tmax=0.)

        # Read forward matrix
        forward = mne.read_forward_solution(fwd_fname)

        # Selecting relevant time window: focusing on early signal
        t_min, t_max = 0.03, 0.05
        t_step = 1.0 / 300

        # We must reduce the whitener since data were preprocessed for removal
        # of environmental noise with maxwell filter leading to an effective
        # number of 64 samples.
        pca = True

        return (subject, subjects_dir, noise_cov, forward, evoked,
                t_min, t_max, t_step, pca)









.. GENERATED FROM PYTHON SOURCE LINES 142-144

The function below preprocess the raw M/EEG data, it notably computes the
whitened MEG/EEG measurements and prepares the gain matrix.

.. GENERATED FROM PYTHON SOURCE LINES 144-210

.. code-block:: default



    def preprocess_meg_eeg_data(evoked, forward, noise_cov, loose=0., depth=0.,
                                pca=False):
        """Preprocess MEG or EEG data to produce the whitened MEG/EEG measurements
        (target) and the preprocessed gain matrix (design matrix). This function
        is mainly wrapping the `_prepare_gain` MNE function.

        Parameters
        ----------
        evoked : instance of mne.Evoked
            The evoked data.

        forward : instance of Forward
            The forward solution.

        noise_cov : instance of Covariance
            The noise covariance.

        loose : float in [0, 1] or 'auto'
            Value that weights the source variances of the dipole components
            that are parallel (tangential) to the cortical surface. If loose
            is 0 then the solution is computed with fixed orientation.
            If loose is 1, it corresponds to free orientations.
            The default value ('auto') is set to 0.2 for surface-oriented source
            space and set to 1.0 for volumic or discrete source space.
            See for details:
            https://mne.tools/stable/auto_tutorials/inverse/35_dipole_orientations.html?highlight=loose

        depth : None or float in [0, 1]
            Depth weighting coefficients. If None, no depth weighting is performed.

        pca : bool, optional (default=False)
            If True, whitener is reduced.
            If False, whitener is not reduced (square matrix).

        Returns
        -------
        G : array, shape (n_channels, n_dipoles)
            The preprocessed gain matrix. If pca=True then n_channels is
            effectively equal to the rank of the data.

        M : array, shape (n_channels, n_times)
            The whitened MEG/EEG measurements. If pca=True then n_channels is
            effectively equal to the rank of the data.

        forward : instance of Forward
            The preprocessed forward solution.
        """

        all_ch_names = evoked.ch_names

        # Handle depth weighting and whitening (here is no weights)
        forward, G, gain_info, whitener, _, _ = \
            _prepare_gain(forward, evoked.info, noise_cov, pca=pca, depth=depth,
                          loose=loose, weights=None, weights_min=None, rank=None)

        # Select channels of interest
        sel = [all_ch_names.index(name) for name in gain_info['ch_names']]

        M = evoked.data[sel]
        M = np.dot(whitener, M)

        return G, M, forward









.. GENERATED FROM PYTHON SOURCE LINES 211-213

The next function translates the solution in a readable format for the
MNE plotting functions that require a Source Time Course (STC) object.

.. GENERATED FROM PYTHON SOURCE LINES 213-228

.. code-block:: default



    def _compute_stc(zscore_active_set, active_set, evoked, forward):
        """Wrapper of `_make_sparse_stc`"""

        X = np.atleast_2d(zscore_active_set)

        if X.shape[1] > 1 and X.shape[0] == 1:
            X = X.T

        stc = _make_sparse_stc(X, active_set, forward, tmin=evoked.times[0],
                               tstep=1. / evoked.info['sfreq'])
        return stc









.. GENERATED FROM PYTHON SOURCE LINES 229-231

The function below will be used to modify the connectivity matrix
to avoid multiple warnings when we run the clustering algorithm.

.. GENERATED FROM PYTHON SOURCE LINES 231-260

.. code-block:: default



    def _fix_connectivity(X, connectivity, affinity):
        """Complete the connectivity matrix if necessary"""

        # Convert connectivity matrix into LIL format
        connectivity = connectivity.tolil()

        # Compute the number of nodes
        n_connected_components, labels = connected_components(connectivity)

        if n_connected_components > 1:

            for i in range(n_connected_components):
                idx_i = np.where(labels == i)[0]
                Xi = X[idx_i]
                for j in range(i):
                    idx_j = np.where(labels == j)[0]
                    Xj = X[idx_j]
                    D = pairwise_distances(Xi, Xj, metric=affinity)
                    ii, jj = np.where(D == np.min(D))
                    ii = ii[0]
                    jj = jj[0]
                    connectivity[idx_i[ii], idx_j[jj]] = True
                    connectivity[idx_j[jj], idx_i[ii]] = True

        return connectivity, n_connected_components









.. GENERATED FROM PYTHON SOURCE LINES 261-266

Downloading data
----------------

After choosing a task, we run the function that loads the data to get
the corresponding evoked, forward and noise covariance matrices.

.. GENERATED FROM PYTHON SOURCE LINES 266-281

.. code-block:: default


    # Choose the experiment (task)
    list_cond = ['audio', 'visual', 'somato']
    cond = list_cond[2]
    print(f"Let's process the condition: {cond}")

    # Load the data
    if cond in ['audio', 'visual']:
        sub, subs_dir, noise_cov, forward, evoked, t_min, t_max, t_step, pca = \
            _load_sample(cond)

    elif cond == 'somato':
        sub, subs_dir, noise_cov, forward, evoked, t_min, t_max, t_step, pca = \
            _load_somato(cond)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Let's process the condition: somato
    Using default location ~/mne_data for somato...
    Creating ~/mne_data
    Downloading archive MNE-somato-data.tar.gz to /home/runner/mne_data
    Downloading https://files.osf.io/v1/resources/rxvq7/providers/osfstorage/59c0e2849ad5a1025d4b7346?version=7&action=download&direct (582.2 MB)
      0%|          | Downloading : 0.00/582M [00:00<?,        ?B/s]      0%|          | Downloading : 120k/582M [00:00<06:20,    1.60MB/s]      0%|          | Downloading : 248k/582M [00:00<06:06,    1.66MB/s]      0%|          | Downloading : 376k/582M [00:00<05:54,    1.72MB/s]      0%|          | Downloading : 504k/582M [00:00<05:42,    1.78MB/s]      0%|          | Downloading : 632k/582M [00:00<05:30,    1.84MB/s]      0%|          | Downloading : 760k/582M [00:00<05:18,    1.91MB/s]      0%|          | Downloading : 888k/582M [00:00<05:06,    1.99MB/s]      0%|          | Downloading : 1.24M/582M [00:00<04:53,    2.08MB/s]      0%|          | Downloading : 1.49M/582M [00:00<04:40,    2.17MB/s]      0%|          | Downloading : 1.74M/582M [00:00<04:28,    2.27MB/s]      0%|          | Downloading : 2.24M/582M [00:00<04:16,    2.37MB/s]      1%|          | Downloading : 2.99M/582M [00:00<04:04,    2.49MB/s]      1%|          | Downloading : 3.99M/582M [00:00<03:52,    2.61MB/s]      1%|          | Downloading : 4.99M/582M [00:00<03:41,    2.74MB/s]      1%|1         | Downloading : 6.49M/582M [00:00<03:30,    2.87MB/s]      1%|1         | Downloading : 8.49M/582M [00:00<03:19,    3.02MB/s]      2%|1         | Downloading : 9.49M/582M [00:00<03:09,    3.17MB/s]      2%|1         | Downloading : 11.5M/582M [00:00<03:00,    3.32MB/s]      2%|2         | Downloading : 12.5M/582M [00:00<02:51,    3.49MB/s]      2%|2         | Downloading : 13.5M/582M [00:00<02:43,    3.66MB/s]      3%|2         | Downloading : 15.5M/582M [00:00<02:34,    3.84MB/s]      3%|2         | Downloading : 16.5M/582M [00:00<02:27,    4.02MB/s]      3%|3         | Downloading : 18.5M/582M [00:00<02:20,    4.22MB/s]      3%|3         | Downloading : 19.5M/582M [00:00<02:13,    4.42MB/s]      4%|3         | Downloading : 20.5M/582M [00:00<02:07,    4.63MB/s]      4%|3         | Downloading : 22.5M/582M [00:00<02:00,    4.85MB/s]      4%|4         | Downloading : 23.5M/582M [00:00<01:55,    5.08MB/s]      4%|4         | Downloading : 24.5M/582M [00:00<01:49,    5.32MB/s]      4%|4         | Downloading : 25.5M/582M [00:00<01:45,    5.55MB/s]      5%|4         | Downloading : 26.5M/582M [00:00<01:40,    5.80MB/s]      5%|4         | Downloading : 27.5M/582M [00:00<01:36,    6.04MB/s]      5%|4         | Downloading : 28.5M/582M [00:00<01:32,    6.30MB/s]      5%|5         | Downloading : 29.5M/582M [00:00<01:28,    6.56MB/s]      5%|5         | Downloading : 30.5M/582M [00:00<01:25,    6.80MB/s]      5%|5         | Downloading : 31.5M/582M [00:01<01:22,    6.98MB/s]      6%|5         | Downloading : 32.5M/582M [00:01<01:20,    7.13MB/s]      6%|5         | Downloading : 33.5M/582M [00:01<01:19,    7.28MB/s]      6%|5         | Downloading : 34.5M/582M [00:01<01:16,    7.54MB/s]      6%|6         | Downloading : 35.5M/582M [00:01<01:12,    7.86MB/s]      6%|6         | Downloading : 36.5M/582M [00:01<01:09,    8.19MB/s]      6%|6         | Downloading : 37.5M/582M [00:01<01:07,    8.51MB/s]      7%|6         | Downloading : 38.5M/582M [00:01<01:04,    8.86MB/s]      7%|6         | Downloading : 39.5M/582M [00:01<01:01,    9.21MB/s]      7%|6         | Downloading : 40.5M/582M [00:01<00:59,    9.58MB/s]      7%|7         | Downloading : 41.5M/582M [00:01<00:56,    9.95MB/s]      7%|7         | Downloading : 42.5M/582M [00:01<00:54,    10.3MB/s]      7%|7         | Downloading : 43.5M/582M [00:01<00:52,    10.7MB/s]      8%|7         | Downloading : 44.5M/582M [00:01<00:50,    11.1MB/s]      8%|7         | Downloading : 45.5M/582M [00:01<00:48,    11.5MB/s]      8%|7         | Downloading : 46.5M/582M [00:01<00:47,    11.8MB/s]      8%|8         | Downloading : 48.5M/582M [00:01<00:45,    12.4MB/s]      9%|8         | Downloading : 50.5M/582M [00:01<00:44,    12.7MB/s]      9%|9         | Downloading : 52.5M/582M [00:01<00:41,    13.3MB/s]      9%|9         | Downloading : 54.5M/582M [00:01<00:40,    13.7MB/s]     10%|9         | Downloading : 56.5M/582M [00:01<00:39,    14.1MB/s]     10%|#         | Downloading : 58.5M/582M [00:01<00:37,    14.7MB/s]     10%|#         | Downloading : 60.5M/582M [00:01<00:36,    14.9MB/s]     11%|#         | Downloading : 62.5M/582M [00:02<00:35,    15.6MB/s]     11%|#1        | Downloading : 64.5M/582M [00:02<00:33,    16.0MB/s]     11%|#1        | Downloading : 66.5M/582M [00:02<00:32,    16.5MB/s]     12%|#1        | Downloading : 68.5M/582M [00:02<00:31,    17.2MB/s]     12%|#2        | Downloading : 70.5M/582M [00:02<00:30,    17.7MB/s]     12%|#2        | Downloading : 72.5M/582M [00:02<00:28,    18.5MB/s]     13%|#2        | Downloading : 74.5M/582M [00:02<00:28,    19.0MB/s]     13%|#3        | Downloading : 76.5M/582M [00:02<00:27,    19.6MB/s]     13%|#3        | Downloading : 78.5M/582M [00:02<00:27,    19.3MB/s]     14%|#3        | Downloading : 79.5M/582M [00:02<00:26,    19.6MB/s]     14%|#3        | Downloading : 80.5M/582M [00:02<00:25,    20.2MB/s]     14%|#3        | Downloading : 81.5M/582M [00:02<00:25,    20.6MB/s]     14%|#4        | Downloading : 82.5M/582M [00:02<00:24,    21.1MB/s]     14%|#4        | Downloading : 83.5M/582M [00:02<00:24,    21.6MB/s]     15%|#4        | Downloading : 84.5M/582M [00:02<00:23,    22.2MB/s]     15%|#4        | Downloading : 85.5M/582M [00:02<00:22,    22.7MB/s]     15%|#4        | Downloading : 86.5M/582M [00:02<00:22,    23.3MB/s]     15%|#5        | Downloading : 87.5M/582M [00:02<00:21,    23.9MB/s]     15%|#5        | Downloading : 88.5M/582M [00:02<00:21,    24.4MB/s]     15%|#5        | Downloading : 89.5M/582M [00:02<00:20,    24.8MB/s]     16%|#5        | Downloading : 91.5M/582M [00:02<00:19,    25.8MB/s]     16%|#6        | Downloading : 93.5M/582M [00:02<00:19,    26.3MB/s]     16%|#6        | Downloading : 95.5M/582M [00:02<00:18,    27.1MB/s]     17%|#6        | Downloading : 97.5M/582M [00:02<00:18,    27.7MB/s]     17%|#7        | Downloading : 99.5M/582M [00:02<00:18,    28.1MB/s]     17%|#7        | Downloading : 101M/582M [00:02<00:17,    28.7MB/s]      18%|#7        | Downloading : 103M/582M [00:03<00:17,    29.2MB/s]     18%|#8        | Downloading : 105M/582M [00:03<00:16,    29.6MB/s]     18%|#8        | Downloading : 107M/582M [00:03<00:16,    30.2MB/s]     19%|#8        | Downloading : 109M/582M [00:03<00:16,    30.6MB/s]     19%|#9        | Downloading : 111M/582M [00:03<00:15,    31.1MB/s]     19%|#9        | Downloading : 113M/582M [00:03<00:15,    31.3MB/s]     20%|#9        | Downloading : 115M/582M [00:03<00:16,    29.7MB/s]     20%|##        | Downloading : 116M/582M [00:03<00:16,    28.8MB/s]     20%|##        | Downloading : 117M/582M [00:03<00:17,    27.9MB/s]     20%|##        | Downloading : 118M/582M [00:03<00:17,    28.4MB/s]     21%|##        | Downloading : 119M/582M [00:03<00:16,    29.1MB/s]     21%|##        | Downloading : 120M/582M [00:03<00:16,    29.4MB/s]     21%|##        | Downloading : 121M/582M [00:03<00:16,    29.5MB/s]     21%|##1       | Downloading : 122M/582M [00:03<00:16,    29.4MB/s]     21%|##1       | Downloading : 123M/582M [00:03<00:16,    29.7MB/s]     21%|##1       | Downloading : 124M/582M [00:03<00:15,    30.0MB/s]     22%|##1       | Downloading : 125M/582M [00:03<00:15,    30.3MB/s]     22%|##1       | Downloading : 126M/582M [00:03<00:15,    31.0MB/s]     22%|##1       | Downloading : 127M/582M [00:03<00:15,    31.7MB/s]     22%|##2       | Downloading : 128M/582M [00:03<00:14,    32.4MB/s]     22%|##2       | Downloading : 129M/582M [00:03<00:14,    33.1MB/s]     22%|##2       | Downloading : 130M/582M [00:03<00:14,    33.7MB/s]     23%|##2       | Downloading : 131M/582M [00:03<00:14,    33.4MB/s]     23%|##2       | Downloading : 132M/582M [00:03<00:14,    32.6MB/s]     23%|##2       | Downloading : 133M/582M [00:03<00:14,    32.9MB/s]     23%|##3       | Downloading : 134M/582M [00:04<00:14,    32.1MB/s]     23%|##3       | Downloading : 135M/582M [00:04<00:14,    32.5MB/s]     24%|##3       | Downloading : 138M/582M [00:04<00:13,    33.2MB/s]     24%|##4       | Downloading : 140M/582M [00:04<00:13,    33.7MB/s]     24%|##4       | Downloading : 142M/582M [00:04<00:13,    33.9MB/s]     25%|##4       | Downloading : 144M/582M [00:04<00:13,    34.6MB/s]     25%|##5       | Downloading : 146M/582M [00:04<00:12,    35.2MB/s]     26%|##5       | Downloading : 148M/582M [00:04<00:12,    36.0MB/s]     26%|##5       | Downloading : 150M/582M [00:04<00:12,    36.4MB/s]     26%|##6       | Downloading : 152M/582M [00:04<00:12,    37.2MB/s]     27%|##6       | Downloading : 154M/582M [00:04<00:11,    37.6MB/s]     27%|##6       | Downloading : 156M/582M [00:04<00:11,    38.4MB/s]     27%|##7       | Downloading : 158M/582M [00:04<00:11,    38.8MB/s]     28%|##7       | Downloading : 160M/582M [00:04<00:11,    39.4MB/s]     28%|##7       | Downloading : 162M/582M [00:04<00:11,    40.0MB/s]     28%|##8       | Downloading : 164M/582M [00:04<00:10,    40.6MB/s]     29%|##8       | Downloading : 166M/582M [00:04<00:10,    41.1MB/s]     29%|##8       | Downloading : 168M/582M [00:04<00:10,    41.5MB/s]     29%|##9       | Downloading : 170M/582M [00:04<00:10,    42.0MB/s]     30%|##9       | Downloading : 172M/582M [00:04<00:10,    42.7MB/s]     30%|##9       | Downloading : 174M/582M [00:04<00:09,    43.2MB/s]     30%|###       | Downloading : 176M/582M [00:04<00:09,    43.4MB/s]     31%|###       | Downloading : 178M/582M [00:04<00:09,    44.0MB/s]     31%|###1      | Downloading : 180M/582M [00:04<00:09,    44.3MB/s]     31%|###1      | Downloading : 182M/582M [00:04<00:09,    44.7MB/s]     32%|###1      | Downloading : 184M/582M [00:05<00:09,    45.2MB/s]     32%|###2      | Downloading : 186M/582M [00:05<00:09,    45.5MB/s]     32%|###2      | Downloading : 188M/582M [00:05<00:10,    39.3MB/s]     33%|###2      | Downloading : 191M/582M [00:05<00:10,    40.7MB/s]     33%|###3      | Downloading : 193M/582M [00:05<00:10,    40.4MB/s]     34%|###3      | Downloading : 195M/582M [00:05<00:09,    41.8MB/s]     34%|###3      | Downloading : 197M/582M [00:05<00:09,    41.5MB/s]     34%|###4      | Downloading : 199M/582M [00:05<00:09,    41.4MB/s]     35%|###4      | Downloading : 201M/582M [00:05<00:09,    42.7MB/s]     35%|###4      | Downloading : 203M/582M [00:05<00:09,    42.6MB/s]     35%|###5      | Downloading : 205M/582M [00:05<00:09,    43.8MB/s]     36%|###5      | Downloading : 207M/582M [00:05<00:08,    43.7MB/s]     36%|###5      | Downloading : 209M/582M [00:05<00:08,    44.7MB/s]     36%|###6      | Downloading : 211M/582M [00:05<00:08,    44.5MB/s]     37%|###6      | Downloading : 213M/582M [00:05<00:08,    45.7MB/s]     37%|###7      | Downloading : 215M/582M [00:05<00:08,    45.5MB/s]     37%|###7      | Downloading : 217M/582M [00:05<00:08,    46.7MB/s]     38%|###7      | Downloading : 219M/582M [00:05<00:08,    46.2MB/s]     38%|###8      | Downloading : 221M/582M [00:05<00:08,    45.9MB/s]     38%|###8      | Downloading : 223M/582M [00:05<00:07,    47.2MB/s]     39%|###8      | Downloading : 225M/582M [00:05<00:07,    46.8MB/s]     39%|###9      | Downloading : 227M/582M [00:05<00:07,    48.1MB/s]     39%|###9      | Downloading : 229M/582M [00:06<00:07,    47.7MB/s]     40%|###9      | Downloading : 231M/582M [00:06<00:07,    46.3MB/s]     40%|####      | Downloading : 233M/582M [00:06<00:07,    46.6MB/s]     40%|####      | Downloading : 235M/582M [00:06<00:07,    46.8MB/s]     41%|####      | Downloading : 237M/582M [00:06<00:09,    36.9MB/s]     41%|####1     | Downloading : 240M/582M [00:06<00:09,    37.3MB/s]     42%|####1     | Downloading : 244M/582M [00:06<00:09,    38.0MB/s]     43%|####2     | Downloading : 248M/582M [00:06<00:09,    38.6MB/s]     43%|####3     | Downloading : 250M/582M [00:06<00:09,    38.5MB/s]     44%|####3     | Downloading : 254M/582M [00:06<00:08,    39.1MB/s]     44%|####4     | Downloading : 256M/582M [00:06<00:08,    38.5MB/s]     44%|####4     | Downloading : 258M/582M [00:06<00:08,    38.0MB/s]     45%|####4     | Downloading : 260M/582M [00:07<00:09,    37.5MB/s]     45%|####5     | Downloading : 262M/582M [00:07<00:08,    38.5MB/s]     45%|####5     | Downloading : 264M/582M [00:07<00:08,    38.8MB/s]     46%|####5     | Downloading : 266M/582M [00:07<00:08,    39.8MB/s]     46%|####6     | Downloading : 268M/582M [00:07<00:08,    40.0MB/s]     46%|####6     | Downloading : 270M/582M [00:07<00:07,    41.1MB/s]     47%|####6     | Downloading : 272M/582M [00:07<00:07,    40.7MB/s]     47%|####7     | Downloading : 276M/582M [00:07<00:07,    41.3MB/s]     48%|####8     | Downloading : 280M/582M [00:07<00:07,    41.9MB/s]     49%|####8     | Downloading : 282M/582M [00:07<00:07,    42.1MB/s]     49%|####8     | Downloading : 284M/582M [00:07<00:07,    43.0MB/s]     49%|####9     | Downloading : 286M/582M [00:07<00:07,    43.2MB/s]     50%|####9     | Downloading : 288M/582M [00:07<00:06,    44.1MB/s]     50%|####9     | Downloading : 290M/582M [00:07<00:06,    44.3MB/s]     50%|#####     | Downloading : 292M/582M [00:07<00:06,    45.4MB/s]     51%|#####     | Downloading : 294M/582M [00:07<00:06,    45.3MB/s]     51%|#####     | Downloading : 296M/582M [00:07<00:06,    46.3MB/s]     51%|#####1    | Downloading : 298M/582M [00:07<00:06,    45.6MB/s]     52%|#####1    | Downloading : 302M/582M [00:07<00:06,    46.1MB/s]     52%|#####2    | Downloading : 304M/582M [00:07<00:06,    46.1MB/s]     53%|#####2    | Downloading : 306M/582M [00:07<00:06,    47.0MB/s]     53%|#####2    | Downloading : 308M/582M [00:07<00:06,    47.0MB/s]     53%|#####3    | Downloading : 310M/582M [00:07<00:05,    48.0MB/s]     54%|#####3    | Downloading : 312M/582M [00:07<00:05,    47.7MB/s]     54%|#####4    | Downloading : 314M/582M [00:08<00:05,    48.8MB/s]     54%|#####4    | Downloading : 316M/582M [00:08<00:05,    48.6MB/s]     55%|#####4    | Downloading : 318M/582M [00:08<00:05,    49.6MB/s]     55%|#####5    | Downloading : 320M/582M [00:08<00:05,    48.6MB/s]     56%|#####5    | Downloading : 324M/582M [00:08<00:05,    48.7MB/s]     56%|#####6    | Downloading : 326M/582M [00:08<00:05,    48.9MB/s]     56%|#####6    | Downloading : 328M/582M [00:08<00:05,    49.3MB/s]     57%|#####6    | Downloading : 330M/582M [00:08<00:05,    49.3MB/s]     57%|#####7    | Downloading : 332M/582M [00:08<00:05,    49.9MB/s]     57%|#####7    | Downloading : 334M/582M [00:08<00:05,    49.4MB/s]     58%|#####7    | Downloading : 336M/582M [00:08<00:05,    50.6MB/s]     58%|#####8    | Downloading : 338M/582M [00:08<00:05,    50.1MB/s]     58%|#####8    | Downloading : 340M/582M [00:08<00:04,    51.1MB/s]     59%|#####8    | Downloading : 342M/582M [00:08<00:04,    50.5MB/s]     59%|#####9    | Downloading : 344M/582M [00:08<00:04,    51.6MB/s]     60%|#####9    | Downloading : 346M/582M [00:08<00:04,    50.9MB/s]     60%|#####9    | Downloading : 348M/582M [00:08<00:04,    50.1MB/s]     60%|######    | Downloading : 350M/582M [00:08<00:04,    51.2MB/s]     61%|######    | Downloading : 352M/582M [00:08<00:04,    50.3MB/s]     61%|######    | Downloading : 354M/582M [00:08<00:04,    51.8MB/s]     61%|######1   | Downloading : 356M/582M [00:08<00:04,    50.8MB/s]     62%|######1   | Downloading : 358M/582M [00:08<00:04,    51.4MB/s]     62%|######1   | Downloading : 360M/582M [00:08<00:04,    51.2MB/s]     62%|######2   | Downloading : 362M/582M [00:08<00:04,    50.7MB/s]     63%|######2   | Downloading : 364M/582M [00:09<00:04,    50.7MB/s]     63%|######2   | Downloading : 366M/582M [00:09<00:04,    50.0MB/s]     63%|######3   | Downloading : 368M/582M [00:09<00:04,    50.2MB/s]     64%|######3   | Downloading : 370M/582M [00:09<00:04,    48.6MB/s]     64%|######3   | Downloading : 372M/582M [00:09<00:04,    48.8MB/s]     64%|######4   | Downloading : 374M/582M [00:09<00:04,    48.7MB/s]     65%|######4   | Downloading : 376M/582M [00:09<00:04,    48.3MB/s]     65%|######5   | Downloading : 378M/582M [00:09<00:04,    46.3MB/s]     65%|######5   | Downloading : 380M/582M [00:09<00:05,    42.1MB/s]     66%|######5   | Downloading : 381M/582M [00:09<00:05,    41.0MB/s]     66%|######5   | Downloading : 382M/582M [00:09<00:05,    40.5MB/s]     66%|######5   | Downloading : 383M/582M [00:09<00:05,    40.2MB/s]     66%|######6   | Downloading : 384M/582M [00:09<00:05,    39.7MB/s]     66%|######6   | Downloading : 385M/582M [00:09<00:05,    40.0MB/s]     66%|######6   | Downloading : 386M/582M [00:09<00:05,    40.4MB/s]     67%|######6   | Downloading : 387M/582M [00:09<00:05,    40.5MB/s]     67%|######6   | Downloading : 388M/582M [00:09<00:04,    40.9MB/s]     67%|######6   | Downloading : 389M/582M [00:09<00:04,    41.4MB/s]     67%|######7   | Downloading : 390M/582M [00:09<00:04,    41.6MB/s]     67%|######7   | Downloading : 391M/582M [00:09<00:04,    41.8MB/s]     68%|######7   | Downloading : 394M/582M [00:09<00:04,    42.2MB/s]     68%|######8   | Downloading : 396M/582M [00:09<00:04,    43.3MB/s]     68%|######8   | Downloading : 398M/582M [00:09<00:04,    43.3MB/s]     69%|######8   | Downloading : 400M/582M [00:10<00:04,    43.5MB/s]     69%|######9   | Downloading : 402M/582M [00:10<00:04,    44.1MB/s]     69%|######9   | Downloading : 404M/582M [00:10<00:04,    44.5MB/s]     70%|######9   | Downloading : 406M/582M [00:10<00:04,    44.4MB/s]     70%|#######   | Downloading : 408M/582M [00:10<00:04,    44.0MB/s]     71%|#######   | Downloading : 410M/582M [00:10<00:04,    43.1MB/s]     71%|#######   | Downloading : 412M/582M [00:10<00:04,    37.5MB/s]     71%|#######1  | Downloading : 415M/582M [00:10<00:04,    38.0MB/s]     72%|#######1  | Downloading : 417M/582M [00:10<00:04,    34.9MB/s]     72%|#######1  | Downloading : 418M/582M [00:10<00:04,    34.5MB/s]     72%|#######2  | Downloading : 419M/582M [00:10<00:04,    34.3MB/s]     72%|#######2  | Downloading : 420M/582M [00:10<00:05,    32.6MB/s]     72%|#######2  | Downloading : 421M/582M [00:10<00:05,    32.3MB/s]     73%|#######2  | Downloading : 422M/582M [00:10<00:05,    31.8MB/s]     73%|#######2  | Downloading : 423M/582M [00:10<00:05,    31.8MB/s]     73%|#######2  | Downloading : 424M/582M [00:10<00:05,    32.1MB/s]     73%|#######3  | Downloading : 425M/582M [00:10<00:05,    32.2MB/s]     73%|#######3  | Downloading : 426M/582M [00:11<00:05,    32.3MB/s]     73%|#######3  | Downloading : 427M/582M [00:11<00:04,    32.5MB/s]     74%|#######3  | Downloading : 428M/582M [00:11<00:04,    32.4MB/s]     74%|#######3  | Downloading : 429M/582M [00:11<00:04,    33.2MB/s]     74%|#######3  | Downloading : 430M/582M [00:11<00:04,    33.7MB/s]     74%|#######4  | Downloading : 432M/582M [00:11<00:04,    34.3MB/s]     75%|#######4  | Downloading : 434M/582M [00:11<00:05,    30.1MB/s]     75%|#######5  | Downloading : 438M/582M [00:11<00:04,    31.2MB/s]     76%|#######5  | Downloading : 440M/582M [00:11<00:04,    31.5MB/s]     76%|#######5  | Downloading : 442M/582M [00:11<00:04,    31.9MB/s]     76%|#######6  | Downloading : 444M/582M [00:11<00:04,    33.0MB/s]     77%|#######6  | Downloading : 446M/582M [00:11<00:04,    33.3MB/s]     77%|#######6  | Downloading : 448M/582M [00:11<00:04,    34.4MB/s]     77%|#######7  | Downloading : 450M/582M [00:11<00:04,    34.4MB/s]     78%|#######7  | Downloading : 452M/582M [00:11<00:03,    35.7MB/s]     78%|#######7  | Downloading : 454M/582M [00:11<00:03,    35.8MB/s]     78%|#######8  | Downloading : 456M/582M [00:11<00:03,    37.1MB/s]     79%|#######8  | Downloading : 458M/582M [00:11<00:03,    37.1MB/s]     79%|#######9  | Downloading : 460M/582M [00:11<00:03,    38.4MB/s]     79%|#######9  | Downloading : 462M/582M [00:11<00:03,    38.4MB/s]     80%|#######9  | Downloading : 464M/582M [00:11<00:03,    38.2MB/s]     80%|########  | Downloading : 466M/582M [00:12<00:03,    38.2MB/s]     80%|########  | Downloading : 468M/582M [00:12<00:03,    38.5MB/s]     81%|########  | Downloading : 470M/582M [00:12<00:03,    39.1MB/s]     81%|########1 | Downloading : 472M/582M [00:12<00:02,    39.6MB/s]     81%|########1 | Downloading : 474M/582M [00:12<00:02,    40.1MB/s]     82%|########1 | Downloading : 476M/582M [00:12<00:02,    40.7MB/s]     82%|########2 | Downloading : 478M/582M [00:12<00:02,    41.4MB/s]     82%|########2 | Downloading : 480M/582M [00:12<00:02,    41.9MB/s]     83%|########2 | Downloading : 482M/582M [00:12<00:02,    41.0MB/s]     83%|########3 | Downloading : 484M/582M [00:12<00:02,    42.4MB/s]     83%|########3 | Downloading : 486M/582M [00:12<00:02,    42.2MB/s]     84%|########3 | Downloading : 488M/582M [00:12<00:02,    41.9MB/s]     84%|########4 | Downloading : 490M/582M [00:12<00:02,    43.0MB/s]     85%|########4 | Downloading : 492M/582M [00:12<00:02,    43.0MB/s]     85%|########4 | Downloading : 494M/582M [00:12<00:02,    44.0MB/s]     85%|########5 | Downloading : 496M/582M [00:12<00:02,    44.1MB/s]     86%|########5 | Downloading : 498M/582M [00:12<00:01,    44.8MB/s]     86%|########5 | Downloading : 500M/582M [00:12<00:01,    44.9MB/s]     86%|########6 | Downloading : 502M/582M [00:12<00:01,    45.7MB/s]     87%|########6 | Downloading : 504M/582M [00:12<00:01,    45.8MB/s]     87%|########6 | Downloading : 506M/582M [00:12<00:01,    46.4MB/s]     87%|########7 | Downloading : 508M/582M [00:12<00:01,    46.5MB/s]     88%|########7 | Downloading : 510M/582M [00:12<00:01,    46.3MB/s]     88%|########7 | Downloading : 512M/582M [00:12<00:01,    47.4MB/s]     88%|########8 | Downloading : 514M/582M [00:12<00:01,    46.7MB/s]     89%|########8 | Downloading : 516M/582M [00:13<00:01,    47.3MB/s]     89%|########8 | Downloading : 518M/582M [00:13<00:01,    47.5MB/s]     89%|########9 | Downloading : 520M/582M [00:13<00:01,    47.7MB/s]     90%|########9 | Downloading : 522M/582M [00:13<00:01,    46.0MB/s]     90%|########9 | Downloading : 524M/582M [00:13<00:01,    45.5MB/s]     90%|######### | Downloading : 526M/582M [00:13<00:01,    46.0MB/s]     91%|######### | Downloading : 528M/582M [00:13<00:01,    46.4MB/s]     91%|#########1| Downloading : 530M/582M [00:13<00:01,    44.8MB/s]     91%|#########1| Downloading : 532M/582M [00:13<00:01,    28.6MB/s]     92%|#########2| Downloading : 536M/582M [00:13<00:01,    29.9MB/s]     93%|#########2| Downloading : 540M/582M [00:14<00:01,    31.3MB/s]     95%|#########4| Downloading : 552M/582M [00:14<00:00,    32.4MB/s]     95%|#########5| Downloading : 556M/582M [00:14<00:00,    32.5MB/s]     96%|#########5| Downloading : 558M/582M [00:14<00:00,    32.3MB/s]     96%|#########6| Downloading : 560M/582M [00:14<00:00,    32.3MB/s]     97%|#########7| Downloading : 566M/582M [00:14<00:00,    32.8MB/s]     98%|#########8| Downloading : 572M/582M [00:14<00:00,    33.7MB/s]     99%|#########8| Downloading : 576M/582M [00:14<00:00,    34.4MB/s]    100%|#########9| Downloading : 580M/582M [00:14<00:00,    35.1MB/s]    100%|##########| Downloading : 582M/582M [00:14<00:00,    35.3MB/s]    100%|##########| Downloading : 582M/582M [00:14<00:00,    41.2MB/s]
    Verifying hash 32fd2f6c8c7eb0784a1de6435273c48b.
    Decompressing the archive: /home/runner/mne_data/MNE-somato-data.tar.gz
    (please be patient, this can take some time)
    Successfully extracted to: ['/home/runner/mne_data/MNE-somato-data']
    Attempting to create new mne-python configuration file:
    /home/runner/.mne/mne-python.json
    Opening raw data file /home/runner/mne_data/MNE-somato-data/sub-01/meg/sub-01_task-somato_meg.fif...
        Range : 237600 ... 506999 =    791.189 ...  1688.266 secs
    Ready.
    111 events found
    Event IDs: [1]
    Not setting metadata
    Not setting metadata
    111 matching events found
    Setting baseline interval to [-0.19979521315838786, 0.0] sec
    Applying baseline correction (mode: mean)
    0 projection items activated
    Loading data for 111 events and 136 original time points ...
    0 bad epochs dropped
    Computing rank from data with rank='info'
        MEG: rank 64 after 0 projectors applied to 306 channels
        Setting small MEG eigenvalues to zero (without PCA)
    Reducing data rank from 306 -> 64
    Estimating covariance using EMPIRICAL
    Done.
    Number of samples used : 6771
    [done]
    Reading forward solution from /home/runner/mne_data/MNE-somato-data/derivatives/sub-01/sub-01_task-somato-fwd.fif...
        Reading a source space...
        [done]
        Reading a source space...
        [done]
        2 source spaces read
        Desired named matrix (kind = 3523) not available
        Read MEG forward solution (8155 sources, 306 channels, free orientations)
        Source spaces transformed to the forward solution coordinate frame




.. GENERATED FROM PYTHON SOURCE LINES 282-287

Preparing data for clustered inference
--------------------------------------

For clustered inference we need the targets ``Y``, the design matrix ``X``
and the ``connectivity`` matrix, which is a sparse adjacency matrix.

.. GENERATED FROM PYTHON SOURCE LINES 287-304

.. code-block:: default


    # Collecting features' connectivity
    connectivity = mne.source_estimate.spatial_src_adjacency(forward['src'])

    # Croping evoked according to relevant time window
    evoked.crop(tmin=t_min, tmax=t_max)

    # Choosing frequency and number of clusters used for compression.
    # Reducing the frequency to 100Hz to make inference faster
    step = int(t_step * evoked.info['sfreq'])
    evoked.decimate(step)
    t_min = evoked.times[0]
    t_step = 1. / evoked.info['sfreq']

    # Preprocessing MEG data
    X, Y, forward = preprocess_meg_eeg_data(evoked, forward, noise_cov, pca=pca)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    -- number of adjacent vertices : 8196
    /home/runner/work/hidimstat/hidimstat/examples/plot_meg_data_example.py:289: RuntimeWarning: 0.5% of original source space vertices have been omitted, tri-based adjacency will have holes.
    Consider using distance-based adjacency or morphing data to all source space vertices.
      connectivity = mne.source_estimate.spatial_src_adjacency(forward['src'])
    Converting forward solution to fixed orietnation
        No patch info available. The standard source space normals will be employed in the rotation to the local surface coordinates....
        Changing to fixed-orientation forward solution with surface-based source orientations...
        [done]
    Computing inverse operator with 204 channels.
        204 out of 306 channels remain after picking
    Selected 204 channels
    Whitening the forward solution.
    Computing rank from covariance with rank=None
        Using tolerance 2.1e-12 (2.2e-16 eps * 204 dim * 46  max singular value)
        Estimated rank (grad): 64
        GRAD: rank 64 computed from 204 data channels with 0 projectors
        Setting small GRAD eigenvalues to zero (without PCA)
    Creating the source covariance matrix
    Adjusting source covariance matrix.




.. GENERATED FROM PYTHON SOURCE LINES 305-312

Running clustered inference
---------------------------

For MEG data ``n_clusters = 1000`` is generally a good default choice.
Taking ``n_clusters > 2000`` might lead to an unpowerful inference.
Taking ``n_clusters < 500`` might compress too much the data leading
to a compressed problem not close enough to the original problem.

.. GENERATED FROM PYTHON SOURCE LINES 312-342

.. code-block:: default


    n_clusters = 1000

    # Setting theoretical FWER target
    fwer_target = 0.1

    # Computing threshold taking into account for Bonferroni correction
    correction_clust_inf = 1. / n_clusters
    zscore_threshold = zscore_from_pval((fwer_target / 2) * correction_clust_inf)

    # Initializing FeatureAgglomeration object used for the clustering step
    connectivity_fixed, _ = \
        _fix_connectivity(X.T, connectivity, affinity="euclidean")
    ward = FeatureAgglomeration(n_clusters=n_clusters, connectivity=connectivity)

    # Making the inference with the clustered inference algorithm
    inference_method = 'desparsified-group-lasso'
    beta_hat, pval, pval_corr, one_minus_pval, one_minus_pval_corr = \
        clustered_inference(X, Y, ward, n_clusters, method=inference_method)

    # Extracting active set (support)
    active_set = np.logical_or(pval_corr < fwer_target / 2,
                               one_minus_pval_corr < fwer_target / 2)
    active_set_full = np.copy(active_set)
    active_set_full[:] = True

    # Translating p-vals into z-scores for nicer visualization
    zscore = zscore_from_pval(pval, one_minus_pval)
    zscore_active_set = zscore[active_set]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Clustered inference: n_clusters = 1000, inference method = desparsified-group-lasso, seed = 0
    /home/runner/.local/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:245: UserWarning: the number of connected components of the connectivity matrix is 2 > 1. Completing it to avoid stopping the tree early.
      connectivity, n_connected_components = _fix_connectivity(
    /home/runner/work/hidimstat/hidimstat/hidimstat/desparsified_lasso.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
      results = np.asarray(results)
    Group reid: AR1 cov estimation




.. GENERATED FROM PYTHON SOURCE LINES 343-347

Visualization
-------------
Now, let us plot the thresholded statistical maps derived thanks to the
clustered inference algorithm referred as cd-MTLasso.

.. GENERATED FROM PYTHON SOURCE LINES 347-389

.. code-block:: default


    # Let's put the solution into the format supported by the plotting functions
    stc = _compute_stc(zscore_active_set, active_set, evoked, forward)

    # Plotting parameters
    if cond == 'audio':
        hemi = 'lh'
        view = 'lateral'
    elif cond == 'visual':
        hemi = 'rh'
        view = 'medial'
    elif cond == 'somato':
        hemi = 'rh'
        view = 'lateral'

    # Plotting clustered inference solution
    mne.viz.set_3d_backend("pyvista")

    if active_set.sum() != 0:
        max_stc = np.max(np.abs(stc.data))
        clim = dict(pos_lims=(3, zscore_threshold, max_stc), kind='value')
        brain = stc.plot(subject=sub, hemi=hemi, clim=clim, subjects_dir=subs_dir,
                         views=view, time_viewer=False)
        brain.add_text(0.05, 0.9, f'{cond} - cd-MTLasso', 'title',
                       font_size=20)

    # Hack for nice figures on HiDimStat website
    save_fig = False
    plot_saved_fig = True
    if save_fig:
        brain.save_image(f'figures/meg_{cond}_cd-MTLasso.png')
    if plot_saved_fig:
        brain.close()
        img = mpimg.imread(f'figures/meg_{cond}_cd-MTLasso.png')
        plt.imshow(img)
        plt.axis('off')

    interactive_plot = False
    if interactive_plot:
        brain = \
            stc.plot(subject=sub, hemi='both', subjects_dir=subs_dir, clim=clim)




.. image-sg:: /auto_examples/images/sphx_glr_plot_meg_data_example_001.png
   :alt: plot meg data example
   :srcset: /auto_examples/images/sphx_glr_plot_meg_data_example_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Using pyvista 3d backend.

    /home/runner/.local/lib/python3.8/site-packages/pyvista/core/dataset.py:1192: PyvistaDeprecationWarning: Use of `point_arrays` is deprecated. Use `point_data` instead.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 390-394

Comparision with sLORETA
------------------------
Now, we compare the results derived from cd-MTLasso with the solution
obtained from the one of the most standard approach: sLORETA.

.. GENERATED FROM PYTHON SOURCE LINES 394-433

.. code-block:: default


    # Running sLORETA with standard hyper-parameter
    lambda2 = 1. / 9
    inv = make_inverse_operator(evoked.info, forward, noise_cov, loose=0.,
                                depth=0., fixed=True)
    stc_full = apply_inverse(evoked, inv, lambda2=lambda2, method='sLORETA')
    stc_full = stc_full.mean()

    # Computing threshold taking into account for Bonferroni correction
    n_features = stc_full.data.size
    correction = 1. / n_features
    zscore_threshold_no_clust = zscore_from_pval((fwer_target / 2) * correction)

    # Computing estimated support by sLORETA
    active_set = np.abs(stc_full.data) > zscore_threshold_no_clust
    active_set = active_set.flatten()

    # Putting the solution into the format supported by the plotting functions
    sLORETA_solution = np.atleast_2d(stc_full.data[active_set]).flatten()
    stc = _make_sparse_stc(sLORETA_solution, active_set, forward, stc_full.tmin,
                           tstep=stc_full.tstep)

    # Plotting sLORETA solution
    if active_set.sum() != 0:
        max_stc = np.max(np.abs(stc.data))
        clim = dict(pos_lims=(3, zscore_threshold_no_clust, max_stc), kind='value')
        brain = stc.plot(subject=sub, hemi=hemi, clim=clim, subjects_dir=subs_dir,
                         views=view, time_viewer=False)
        brain.add_text(0.05, 0.9, f'{cond} - sLORETA', 'title', font_size=20)

        # Hack for nice figures on HiDimStat website
        if save_fig:
            brain.save_image(f'figures/meg_{cond}_sLORETA.png')
        if plot_saved_fig:
            brain.close()
            img = mpimg.imread(f'figures/meg_{cond}_sLORETA.png')
            plt.imshow(img)
            plt.axis('off')




.. image-sg:: /auto_examples/images/sphx_glr_plot_meg_data_example_002.png
   :alt: plot meg data example
   :srcset: /auto_examples/images/sphx_glr_plot_meg_data_example_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Computing inverse operator with 204 channels.
        204 out of 204 channels remain after picking
    Selected 204 channels
    Whitening the forward solution.
    Computing rank from covariance with rank=None
        Using tolerance 2.1e-12 (2.2e-16 eps * 204 dim * 46  max singular value)
        Estimated rank (grad): 64
        GRAD: rank 64 computed from 204 data channels with 0 projectors
        Setting small GRAD eigenvalues to zero (without PCA)
    Creating the source covariance matrix
    Adjusting source covariance matrix.
    Computing SVD of whitened and weighted lead field matrix.
        largest singular value = 2.06293
        scaling factor to adjust the trace = 2.84039e+19 (nchan = 204 nzero = 140)
    Preparing the inverse operator for use...
        Scaled noise and source covariance from nave = 1 to nave = 111
        Created the regularized inverter
        The projection vectors do not apply to these channels.
        Created the whitener using a noise covariance matrix with rank 64 (140 small eigenvalues omitted)
        Computing noise-normalization factors (sLORETA)...
    [done]
    Applying inverse operator to "1"...
        Picked 204 channels from the data
        Computing inverse...
        Eigenleads need to be weighted ...
        Computing residual...
        Explained  95.4% variance
        sLORETA...
    [done]
    /home/runner/.local/lib/python3.8/site-packages/pyvista/core/dataset.py:1192: PyvistaDeprecationWarning: Use of `point_arrays` is deprecated. Use `point_data` instead.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 434-442

Analysis of the results
-----------------------
While the clustered inference solution always highlights the expected
cortex (audio, visual or somato-sensory) with a universal predertemined
threshold, the solution derived from the sLORETA method does not enjoy
the same property. For the audio task the method is conservative and
for the somato task the method makes false discoveries (then it seems
anti-conservative).

.. GENERATED FROM PYTHON SOURCE LINES 445-452

Running ensemble clustered inference
------------------------------------

To go further it is possible to run the ensemble clustered inference
algorithm. It might take several minutes on standard device with
``n_jobs=1`` (around 10 min). Just set
``run_ensemble_clustered_inference=True`` below.

.. GENERATED FROM PYTHON SOURCE LINES 452-495

.. code-block:: default

    run_ensemble_clustered_inference = False

    if run_ensemble_clustered_inference:
        # Making the inference with the ensembled clustered inference algorithm
        beta_hat, pval, pval_corr, one_minus_pval, one_minus_pval_corr = \
            ensemble_clustered_inference(X, Y, ward, n_clusters,
                                         inference_method=inference_method)

        # Extracting active set (support)
        active_set = np.logical_or(pval_corr < fwer_target / 2,
                                   one_minus_pval_corr < fwer_target / 2)
        active_set_full = np.copy(active_set)
        active_set_full[:] = True

        # Translating p-vals into z-scores for nicer visualization
        zscore = zscore_from_pval(pval, one_minus_pval)
        zscore_active_set = zscore[active_set]

        # Putting the solution into the format supported by the plotting functions
        stc = _compute_stc(zscore_active_set, active_set, evoked, forward)

        # Plotting ensemble clustered inference solution
        if active_set.sum() != 0:
            max_stc = np.max(np.abs(stc._data))
            clim = dict(pos_lims=(3, zscore_threshold, max_stc), kind='value')
            brain = stc.plot(subject=sub, hemi=hemi, clim=clim,
                             subjects_dir=subs_dir, views=view,
                             time_viewer=False)
            brain.add_text(0.05, 0.9, f'{cond} - ecd-MTLasso',
                           'title', font_size=20)

            # Hack for nice figures on HiDimStat website
            if save_fig:
                brain.save_image(f'figures/meg_{cond}_ecd-MTLasso.png')
            if plot_saved_fig:
                brain.close()
                img = mpimg.imread(f'figures/meg_{cond}_ecd-MTLasso.png')
                plt.imshow(img)
                plt.axis('off')

            if interactive_plot:
                brain = stc.plot(subject=sub, hemi='both',
                                 subjects_dir=subs_dir, clim=clim)








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  49.418 seconds)

**Estimated memory usage:**  379 MB


.. _sphx_glr_download_auto_examples_plot_meg_data_example.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_meg_data_example.py <plot_meg_data_example.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_meg_data_example.ipynb <plot_meg_data_example.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
